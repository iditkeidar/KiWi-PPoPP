\section{Evaluation}
\label{sec:eval}

\subsection{Setup}

{\bf Implementation.} We implement {\kiwi} in Java, using Doug Lea's concurrent skip-list 
implementation~\cite{JavaConcurrentSkipList} for the index with added locks to support conditional updates. 
The code makes extensive use of efficient 
array copy methods~\cite{JavaArrayCopy}. {\kiwi}'s chunk size is set to 1024. 
We have found this chunk size to be big enough to offer good locality. Bigger chunks favor puts 
albeit with diminishing returns, and mildly slow down gets and scans because larger chunks 
allow more ``imbalance''  (obsolete data  to iterate through, long bypasses in the search) than smaller ones.

The rebalance policy is tuned as follows:
$\code{checkRebalance}$  invokes rebalance with probability $0.15$ whenever
the batched prefix consists of less than $0.625$ of the linked list. Rebalance 
engages the next chunk whenever doing so will reduce the number of chunks in the list. 
We did not implement the piggybacking of puts on rebalance, and instead restart the put after every rebalance.
%This does not violate lock-freedom since the number of threads is much smaller than the chunk size,  
%hence it is impossible for pending puts to fill it up. %a whole chunk.

The probability for invoking rebalance and the threshold on the ratio of the batched prefix control the aggressiveness of rebalancing. 
On the one hand, aggressive rebalancing reduces put throughput because put threads shoulder the burden of rebalancing. 
On the other hand, it favors gets and scans because it shortens search times and reduces iteration through deleted or over-written items. 
We have chosen these parameters so as to strike a balance between the two. 

\begin{figure*}
\begin{center}
\input{plots/basics}
\end{center}
\caption{Throughput scalability with uniform workloads, 1M dataset. (a) Get operations, (b) Put operations, (c) Scan operations. }
\label{evaluation:results:getputscan}
\end{figure*}

\begin{figure*}
\begin{center}
\input{plots/scans}
\end{center}
\caption{Throughput scalability with concurrent scans and puts. (a,b) Scan operations, 1M dataset. 
(c) Scan operations, 10M dataset. (d,e) Put operations, 1M dataset. (f) Put operations, 10M dataset.  }
\label{evaluation:results:scan}
\end{figure*}

\textbf{Methodology.}
We leverage the popular {\em synchrobench}  microbenchmark~\cite{Gramoli2015}
to exercise a variety of workloads. The hardware platform 
features four Intel Xeon E5-4650 8-core CPUs. %(i.e., 32 hardware threads overall). %hyperthreading disabled). 
Every experiment starts with 20 seconds of {\em warmup} -- inserts
and deletes of random keys -- to let the HotSpot compiler optimizations take effect. 
It then runs 10  iterations, 5 seconds each, and averages the results. An iteration fills the map with 
random (integer, integer) pairs, then exercises some workload. 
Most experiments start from 1M pairs, except those focusing on high scalability that start from 10M.

For the most part, we use the same methodology as previous works, which is the methodology supported by synchrobench, where keys are selected uniformly at random. 
We additionally study an ordered workload to illustrate the benefit of using a balanced data structure as opposed to an unbalanced one. 
We also experimented with a skewed distribution, where after populating the data structure with a uniform distribution over the full key range in the warmup phase, the measured experiments exercise only 10\% of the key range. The results were very similar (in absolute number as well as in trends) to the uniform distribution. For example, in one set of experiments with 4 update threads and 4 threads running scans of different ranges (10 to 25K keys), the put throughput in the skewed key distribution was within 2\% of the put throughput in the uniform one across all scan sizes. Scan times exhibited higher variability -- up to 15\% -- but did not show a clear advantage in one key distribution over the other. Rather, in some experiments the scan was faster over the skewed distribution and vice versa. 

\textbf{Competition.}
We compare {\kiwi} to Java implementations of three concurrent KV-maps: (1) the traditional 
skip-list~\cite{JavaConcurrentSkipList} which does not provide linearizable scan semantics, 
(2) {\snaptree}\cite{BronsonCCO2010}\footnote{\small{\url{https://github.com/nbronson/snaptree}}.}, 
and (3) {\kary}~\cite{BrownA12}\footnote{\small{\url{http://www.cs.toronto.edu/~tabrown/kstrq/LockFreeKSTRQ.java}}.}. 
%The implementations of {\kary} and {\snaptree} are available online. 
For the latter, we use the optimal configuration described in~\cite{BrownA12} with $k=64$. 

\subsection{Results}

\textbf{Basic scenarios: get, put, and scan.} 
We first focus on three simple workloads: 
(1) get-only (random reads), 
(2) put-only (random writes, half inserts/updates and half deletes), and 
(3) scan-only (sequential reads of 32K keys, each starting from a random lower bound).  
%The keys that define the operations are selected uniformly at random. 

Figure~\ref{evaluation:results:getputscan} depicts throughput scalability with the number of worker threads. 
In get-only scenarios (Figure~\ref{evaluation:results:getputscan}(a)), {\kiwi} outperforms the other 
algorithms by 1.25x to 2.5x. We explain this by the NUMA- and cache-friendly locality of access in its intra-chunk binary search. 
Under put-only workloads (Figure~\ref{evaluation:results:getputscan}(b)), it also performs well, thanks to avoiding version
manipulation. {\snaptree}, which is optimized for random writes, is approximately $10\%$ faster than {\kiwi}
with 32 threads. Note that in general, {\kiwi}'s gets are faster than its puts because the latter occasionally incur rebalancing. 

{\kiwi} excels in scan performance (Figure~\ref{evaluation:results:getputscan}(c)). 
For example, with 32 threads, it exceeds its closest competitor, {\kary}, by over $40\%$. 
Here too, {\kiwi}'s advantage stems from high locality of access while scanning big chunks. 

\textbf{Concurrent scans and puts.}
We now turn to the scenario that combines scan operations with 
real-time updates (put operations). This is the primary use case that motivated 
the design principles behind {\kiwi}. Half of the threads perform scans, whereas 
the second half performs puts. 

Figure~\ref{evaluation:results:scan}(a) depicts scan throughput scalability with the number of threads
while scanning ranges of 32K keys. Figure~\ref{evaluation:results:scan}(b) depicts the throughput for 16 scan 
threads with varying range sizes. Note that for long scans, {\kary}'s performance deteriorates under contention. 
This happens because {\kary} restarts the scan every time a put conflicts with it -- i.e., puts make progress 
but scans get starved. For large ranges, {\snaptree} has the second-fastest scans because it shared-locks 
the scanned ranges in advance and iterates unobstructed. 
Note that {\kiwi}'s throughput slightly decreases when the range is particularly big 
because it takes longer to collect redundant versions, and therefore the scan has to sift through 
more data. Figure~\ref{evaluation:results:scan}(c) depicts similar phenomena for a 10M-key dataset.
%
{\snaptree}'s competitive scan performance comes at the expense of puts, 
since its locking approach starves concurrent updates. Figures~\ref{evaluation:results:scan}(d-f) 
 illustrate this behavior -- the latter for a 10M-key dataset.

We study the memory footprints of the solutions in this scenario. We focus on 32-key scans -- a setting 
in which the throughput achieved by all the algorithms except {\snaptree} is similar.  
Figure~\ref{evaluation:results:mem} depicts the JVM memory-in-use metric immediately after a full 
garbage collection that cleans up all the unused objects, averaged across 50 data points. 
{\kiwi} is on par with {\kary} and the Java skiplist except with maximal parallelism (16 put threads), 
in which it consumes 20\% more RAM due to intensive version management.  


%Summing up, contrast to its competitors that are tailored for certain workloads, 
%{\kiwi} serves both scans and puts equally well, echoing the theoretical results.  

\textbf{Ordered workload.} 
As a balanced data structure, {\kiwi} provides good performance on non-random workloads. 
We experiment with a monotonically ordered stream of keys. {\kiwi} achieves a throughput 
similar to the previous experiments. In contrast, {\kary}'s maximal put throughput in this setting is 
730 times slower -- approximately $13.6$K operations/sec vs {\kiwi}'s $9.98$M. 

\remove{
\begin{figure}
\begin{center}
\input{plots/puts-scans}
\end{center}
\caption{Puts with parallel scans, 1M dataset. }
\label{evaluation:results:put-scan}
\end{figure}

}

\begin{figure}
\begin{center}
\input{plots/mem-scans-puts}
\end{center}
\caption{RAM use with parallel scans and puts, 1M dataset. }
\label{evaluation:results:mem}
\end{figure}